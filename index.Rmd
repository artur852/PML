---
title: "Data Analysis"
author: "Artur Costa"
date: "June 11, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Analysis

The original data contained 160 variables that could be used for prediction. Some of these were very straight forward to remove like the name and the timestamps.
Taking the information from the codebook and discussions in the forum, I found out that some of the variables were summary statistics which had a lot of NA values and so I also cut those from the model. Those were prefixed by: "var_", "stddev_", "avg_", "max_", "min_", "amplitude_", "kurtosis_" and "skewness_".

The chosen method was the **Random Forest**, since this was a classification problem with more than two classes, and this algorithm was well exemplified during the lectures and applying 5-fold cross-validation during training. For pre-processing, I used "PCA" and removed columns with near-zero variance.

The final model showed an **accuracy** of 0.9778 and a **Kappa** of 0.9719, which I deemed to be a good anough for this assigment and the number of instances that should be predicted.

In the end, this model presented an estimate for the out-of-sample error of **1.79%**

